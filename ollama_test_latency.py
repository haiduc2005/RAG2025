import time
from langchain_ollama import OllamaLLM

# ã“ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ä»¥ä¸‹ã‚’æ˜Žç¢ºã«æ¯”è¼ƒã§ãã¾ã™ã€‚
# ãƒ»ãƒ¢ãƒ‡ãƒ«ã®åˆå›žãƒ­ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰æ™‚é–“
# ãƒ»2å›žç›®ã®å‘¼ã³å‡ºã—ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¾Œï¼‰æ™‚é–“
# ãƒ»è¤‡æ•°å›žã®å‘¼ã³å‡ºã—æ™‚é–“

# Ollamaãƒ¢ãƒ‡ãƒ«è¨­å®š
LLM_MODEL = "deepseek-r1:1.5b"
OLLAMA_BASE_URL = "http://localhost:11434"  # OllamaãŒå®Ÿè¡Œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„

# LLMã®åˆæœŸåŒ–ï¼ˆæ³¨æ„ï¼šã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ãƒ¢ãƒ‡ãƒ«ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ï¼‰
llm = OllamaLLM(
    model=LLM_MODEL,
    temperature=0.7,
    base_url=OLLAMA_BASE_URL,
    max_tokens=50
)

# ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆï¼šåˆå›žå‘¼ã³å‡ºã—
print("ðŸ§Š Cold start...")
start = time.time()
response = llm.invoke("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(f"å“åº”å†…å®¹: {response}")
print(f"â±ï¸ å†·å¯åŠ¨è€—æ—¶: {time.time() - start:.2f} ç§’")

# ã—ã°ã‚‰ãä¸€æ™‚åœæ­¢ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå®Œå…¨ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
time.sleep(3)

# ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆï¼šå†å‘¼ã³å‡ºã—
print("\nðŸ”¥ Warm start...")
start = time.time()
response = llm.invoke("ä½ æ˜¯è°ï¼Ÿ")
print(f"å“åº”å†…å®¹: {response}")
print(f"â±ï¸ çƒ­å¯åŠ¨è€—æ—¶: {time.time() - start:.2f} ç§’")

# è¤‡æ•°å›žå‘¼ã³å‡ºã—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("\nðŸ” å¤šè½®æµ‹è¯•...")
questions = ["ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "ä½ æ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿ", "ä½ çŸ¥é“ä»€ä¹ˆæ˜¯RAGå—ï¼Ÿ"]
for q in questions:
    start = time.time()
    r = llm.invoke(q)
    print(f"[{q}] -> {time.time() - start:.2f} ç§’")