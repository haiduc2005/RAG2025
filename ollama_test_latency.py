import time
from langchain_ollama import OllamaLLM

# æµ‹è¯•è„šæœ¬ï¼Œèƒ½å¤Ÿæ¸…æ™°å¯¹æ¯”ï¼š
# ãƒ»æ¨¡åž‹é¦–æ¬¡åŠ è½½ï¼ˆå†·å¯åŠ¨ï¼‰æ—¶é—´
# ãƒ»ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆé¢„çƒ­åŽï¼‰æ—¶é—´
# ãƒ»å¤šè½®è°ƒç”¨æ—¶é—´

# Ollama æ¨¡åž‹è®¾ç½®
LLM_MODEL = "deepseek-r1:1.5b"
OLLAMA_BASE_URL = "http://localhost:11434"  # ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ

# åˆå§‹åŒ– LLMï¼ˆæ³¨æ„ï¼Œè¿™ä¸€æ­¥ä¸ä¼šåŠ è½½æ¨¡åž‹ï¼‰
llm = OllamaLLM(
    model=LLM_MODEL,
    temperature=0.7,
    base_url=OLLAMA_BASE_URL,
    max_tokens=50
)

# å†·å¯åŠ¨ï¼šé¦–æ¬¡è°ƒç”¨
print("ðŸ§Š Cold start...")
start = time.time()
response = llm.invoke("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(f"å“åº”å†…å®¹: {response}")
print(f"â±ï¸ å†·å¯åŠ¨è€—æ—¶: {time.time() - start:.2f} ç§’")

# æš‚åœä¸€ä¼šï¼ˆç¡®ä¿æ¨¡åž‹å·²åŠ è½½å®Œï¼‰
time.sleep(3)

# çƒ­å¯åŠ¨ï¼šå†æ¬¡è°ƒç”¨
print("\nðŸ”¥ Warm start...")
start = time.time()
response = llm.invoke("ä½ æ˜¯è°ï¼Ÿ")
print(f"å“åº”å†…å®¹: {response}")
print(f"â±ï¸ çƒ­å¯åŠ¨è€—æ—¶: {time.time() - start:.2f} ç§’")

# å¤šè½®è°ƒç”¨æ¨¡æ‹Ÿ
print("\nðŸ” å¤šè½®æµ‹è¯•...")
questions = ["ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "ä½ æ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿ", "ä½ çŸ¥é“ä»€ä¹ˆæ˜¯RAGå—ï¼Ÿ"]
for q in questions:
    start = time.time()
    r = llm.invoke(q)
    print(f"[{q}] -> {time.time() - start:.2f} ç§’")